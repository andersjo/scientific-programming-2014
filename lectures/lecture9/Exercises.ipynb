{
 "metadata": {
  "name": "",
  "signature": "sha256:3cf4dfcb3cec7c3897c019fcc181b4a7413bea98a291f940c1c81c5ef78d435e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from itertools import chain\n",
      "\n",
      "np.set_printoptions(precision=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercises: Random Sentence Generator\n",
      "====\n",
      "\n",
      "Typically, Markov Chains are used to predict future states, but we can also use it generatively (Markov Chains are a generative graphical model).\n",
      "\n",
      "Andrey Markov first used Markov Chains to predict the next letter in a novel. We will do something similar today and use a Markov Chain to predict the next word, i.e., to generate sentences. \n",
      "\n",
      "This technique is a bit infamous, since it was used by some people to generate real-looking, but meaningless papers (that got accepted!), and is still used by spammers, in order to generate their emails.\n",
      "\n",
      "We don't want to educate you in becoming a shady scientist or business person: Markov Chains are also the basis of HMMs (Hidden Markov Models) and CRFs (Conditional Random Fields), which are widely used in speech recognition and NLP (and thus in Language Processing 2...)\n",
      "In addition, Markov Chains are used in language models to score the outcome of Machine Translation systems.\n",
      "\n",
      "We get the probability information from counting the words in a corpus. Here is a little toy example. We use the `Counter` object to get the word counts.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\"i can do that .\".split(), \"i will code markov_chains .\".split(), \"i can learn markov_chains .\".split()]\n",
      "print corpus\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['i', 'can', 'do', 'that', '.'], ['i', 'will', 'code', 'markov_chains', '.'], ['i', 'can', 'learn', 'markov_chains', '.']]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each word is a state, and since we use vectors and matrices, we need to be able to map between words and tehir position in the vector/matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of all unique words and store them for later lookup\n",
      "words = list(reduce(set.union, map(set, list(corpus))))\n",
      "word2int = dict((w, i) for (i, w) in enumerate(words))\n",
      "N = len(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have everything in place to generate our sentence.\n",
      "\n",
      "The generator starts out by picking a first word proportianate to the number of times we have seen this word start a sentence: if we have seen it a lot, the generator will choose it more often. The probability to choose the first word depends on nothing, so the first state has simply the probability $P(first)$. (If you would like to have only conditional probabilities in your model, you can imagine the probability of picking the first word as $P(first|START)$, where $START$ is a special token.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect all first words from the corpus\n",
      "first = np.zeros(N)\n",
      "for word, frequency in Counter(sentence[0] for sentence in corpus).iteritems():\n",
      "    first[word2int[word]] = frequency\n",
      "    \n",
      "# normalize\n",
      "first /= first.sum()\n",
      "\n",
      "first"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's not a very interesting distribution, since all sentences started with \"`i`\", but it'll do.\n",
      "\n",
      "All subsequent words are generated with probability $P(w_{t+1}|w_t)$, where $t$ is the *time step*, or position in the sentence. These probabilities are also called *transition* probabilities. If we only base them on the current word, we are essentially using a two-word window, so people say that we use a *first-order* Markov Chain, or - especially for words - a *bigram* chain.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# join all sentences into one long word list\n",
      "word_list = list(chain.from_iterable(corpus))\n",
      "bigrams = np.zeros((N,N))\n",
      "for (word1, word2), frequency in Counter(nltk.bigrams(word_list)).iteritems():\n",
      "    bigrams[word2int[word1], word2int[word2]] = frequency\n",
      "\n",
      "# normalize each row\n",
      "row_sums = bigrams.sum(axis=1)\n",
      "bigrams = bigrams / row_sums[:, np.newaxis]\n",
      "\n",
      "bigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "array([[ 0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
        "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ],\n",
        "       [ 0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
        "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.333,  0.667,  0.   ,  0.   ],\n",
        "       [ 0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
        "       [ 0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
        "       [ 0.5  ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.5  ,  0.   ],\n",
        "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ],\n",
        "       [ 0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the matrix is pretty sparse: we haven't seen most of the possible bigrams.\n",
      "\n",
      "Unlike in the weather example, we do not want to simply pick the most likely word at each step, but we want to pick a word according to the distribution over words. I.e., if 50% of the time, \"I\" is followed by \"like\", then we want to pick \"like\" 50% of the time we run our generator, but we also want to pick the other possible words proportionately. \n",
      "Otherwise, our generator would always produce the same sentence. We use the `numpy.random.multinomial` function to pass it a vector of probabilities and sample according to the distribution.\n",
      "\n",
      "We can now define our chain to sample from the different distributions we have: the initial word distribution, and the conditional probability distribution over the bigrams."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run the chain\n",
      "print \"Here's your mantra:\"\n",
      "print\n",
      "\n",
      "# start out with the distribution over first words\n",
      "current = first\n",
      "print words[np.argmax(np.random.multinomial(1, current))],\n",
      "\n",
      "# sample and print the chosen word\n",
      "for i in xrange(19):\n",
      "    current = np.dot(current, bigrams)\n",
      "    w = np.argmax(np.random.multinomial(1, current))\n",
      "    print words[w],\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Here's your mantra:\n",
        "\n",
        "i can do markov_chains . i can code markov_chains . i will code markov_chains . i will learn markov_chains .\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 1\n",
      "----\n",
      "\n",
      "The toy example is not very interesting. Let's work with some real data. We'll use the NLTK version of the Brown corpus. rather than plain matrices and mapping to integers, we'll use `pandas` (this'll also make it easier to go to higher order chains)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use only the first 20k words and lower-case them\n",
      "corpus = map(str.lower, brown.words()[:20000])\n",
      "\n",
      "# get counts\n",
      "first_word_count = Counter([sentence[0].lower() for sentence in brown.sents()[:100]])\n",
      "first_words, first_counts = zip(*first_word_count.items())\n",
      "P_first = pd.DataFrame(data=[first_words, first_counts]).T\n",
      "P_first.set_index(P_first[0])\n",
      "\n",
      "bigram_counts = Counter(nltk.bigrams(corpus))\n",
      "w1, w2 = zip(*bigram_counts.keys())\n",
      "P_bigram = pd.DataFrame(data=[w1, w2, map(float, bigram_counts.values())]).T\n",
      "P_bigram.set_index(P_bigram[0])\n",
      "\n",
      "final_words = Counter([sentence[-1].lower() for sentence in brown.sents()[:100]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sample_first():\n",
      "    total = float(sum(P_first[1]))\n",
      "    P_first[1] /= total\n",
      "    \n",
      "    x = np.argmax(np.random.multinomial(1, np.array(P_first[1], dtype=float)))\n",
      "    return P_first[0][x]\n",
      "\n",
      "\n",
      "def sample_next_bigram_word(word):\n",
      "    #get distro\n",
      "    column = P_bigram[P_bigram[0] == word]\n",
      "    total = float(sum(column[2]))\n",
      "    column[2] = column[2] / total\n",
      "    \n",
      "    # sample from distribution\n",
      "    x = np.argmax(np.random.multinomial(1, np.array(column[2], dtype=float)))\n",
      "    return column[1].values[x]\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can run our code and generate a number of sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate():\n",
      "    result = []\n",
      "    previous_word = None\n",
      "    word = sample_first()\n",
      "    result.append(word)\n",
      "    while word not in final_words:\n",
      "        next_word = sample_next_bigram_word(word)\n",
      "\n",
      "        previous_word = word\n",
      "        word = next_word\n",
      "\n",
      "        result.append(word)\n",
      "\n",
      "    return ' '.join(result)\n",
      "\n",
      "for x in xrange(5):\n",
      "    print x+1, generate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the problem of last three factors in his advocacy of 1961 , 1945 , i am taking the schools , president kennedy today jones against the patient .\n",
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hartsfield , it urged new administration would carry a political scientist writes of house aids in the polls , which may be faced this matter what stand for the required to 1.5 billion or deny the bill , sheets added , the park hotel and the increased federal grants to meet absolutely essential to attend the council that the house .\n",
        "3 `` seek a border patrolman and tougher than to prince souvanna phouma , while in fulton county is worth very modest beginning in the defense counsel for work .\n",
        "4 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "a tax authorities on expansion joints along pennsylvania avenue , even us sens. james cotten of the presidential election were reviewed and i don't work for a patient could tell them would not the kennedy has left little doubt that .\n",
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "davis received 1,119 votes would increase in the park hotel owner said , is 4.4 million dollar a lawyer with principles that voting precincts to reduce the project were not met in mind at the republican party called racial discrimination in `` are the soviet union .\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "The results are somewhere at the intriguing boundary between sensible and complete gibberish (sounds like spam email?). The problem is that the generator only looks at two words at a time (the bigram), and that many phrases and constructions span more than two words. Consequently, the generated sentences \"switch direction\" in mid-stream.\n",
      "\n",
      "The best way to improve this is to include more information about the past than just the current word. How about the previous one as well? If we condition the next word on the previous and the current word, we are using a *trigram* or *second-order* Markov Chain.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 1:\n",
      "----\n",
      "\n",
      "The generate sentences stop whenever they see a word that was the last in a sentence. This is a pretty hard criterion. Implement a probabilistic version. I.e., if the current word is among the final words, quit with probability proportionate to the times this word actually ends a sentence, and continue with probability proportionate to the times the word is somewhere in the middle of a sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate2():\n",
      "    result = []\n",
      "    # Your code here\n",
      "        \n",
      "    return ' '.join(result)\n",
      "\n",
      "for x in xrange(5):\n",
      "    print x+1, generate2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 2:\n",
      "----\n",
      "\n",
      "Modify the bigram code from above to get trigram statistics and write a function to sample from it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "trigram_counts = Counter(nltk.ngrams(corpus, 3))\n",
      "\n",
      "def sample_next_trigram_word(word1, word2):\n",
      "    # Your code here\n",
      "    return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 3:\n",
      "----\n",
      "\n",
      "The advantage of trigrams is that it will generate much more sensible text. The disadvantage is that we will never see every combination of three words possible. We can thus get into a situation where we try to condition on the current and previous word, but there exist no statistics on that particular combination. This is called *sparsity*. In order to avoid the generator from breaking, we need to be able to fall back on bigrams whenever the trigram sampling fails.\n",
      "\n",
      "Modify the generator in order to include trigrams, and make sure it can handle the sparsity problem. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate3():\n",
      "    result = []\n",
      "    # Your code here\n",
      "        \n",
      "    return ' '.join(result)\n",
      "\n",
      "for x in xrange(5):\n",
      "    print x+1, generate3()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 \n",
        "2 \n",
        "3 \n",
        "4 \n",
        "5 \n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 4:\n",
      "----\n",
      "\n",
      "A side effect of the sparse trigram matrix (and even more so of higher-order chains) is that the generator might end up faithfully replicating large parts of the original sentence, simply because for any two-word history, there is only one option how to continue (not too many words can follow the bigram \"*appelate court*\"). One way around this is to use even more data, i.e., even larger corpora. \n",
      "\n",
      "However, there is another way to overcome this sparsity, namely ***pseudo-counts***, also called smoothing or, more general, regularization.\n",
      "\n",
      "We can make our transition matrix less sparse by adding counts to each cell. This flattens out (or \"smoothes\") the distribution.\n",
      "\n",
      "Implement your smoothing option with different values (try 1, 2, 5, 10, and 100) and see what happens. What if you use different values for bigram and trigram smoothing?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sample_next_bigram_word(word, smoothing=1):\n",
      "    # Your code here\n",
      "    return None\n",
      "\n",
      "def sample_next_trigram_word(word1, word2, smoothing=1):\n",
      "    # Your code here\n",
      "    return None\n",
      "    \n",
      "def generate4():\n",
      "    result = []\n",
      "    # Your code here\n",
      "    return ' '.join(result)\n",
      "\n",
      "for x in xrange(5):\n",
      "    print x+1, generate4()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 \n",
        "2 \n",
        "3 \n",
        "4 \n",
        "5 \n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}