{
 "metadata": {
  "name": "",
  "signature": "sha256:e61d35443abe4404e27ff543bb3da676fd6d865b6b2482c201754d901e4cfd5b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Markov Chains\n",
      "====\n",
      "\n",
      "Markov Chains are a special kind of graph that describe how how a system changes between different states. The states are the nodes of the graph, and the transitions between them are the edges. The edges are weighted, and expressed as conditional probability, i.e., what is the chance of going into the next state, given the state I am currently in. This is called the *Markov assumption*, i.e., that you don't take all the previous states into account.\n",
      "\n",
      "Some state transitions are very likely, while others are not likely or even impossible. The sum of the weights of all edges leaving a node has to sum to one.\n",
      "\n",
      "Markov Chains are named after a Russian Mathematician, Andrey Markov, who first used it to predict the next letter in a novel. We will do something similar today and use a Markob Chain to generate sentences. This technique is a bit infamous, since it was used by some people to generate real-looking, but meaningless papers (that got accepted!), and by spammers, in order to generate their emails.\n",
      "\n",
      "We don't want to educate you in becoming a shady scientist or business person: Markov Chains are also the basis of HMMs (Hidden Markov Model), which are widely used in speech recognition and (to a lesser extent) in NLP.\n",
      "\n",
      "The generator starts out by picking a first word proportianate to the number of times we have seen this word start a sentence: if we have seen it a lot, the generator will choose it more often. The probability to choose the first word depends on nothing, so the first state has simply the probability $P(first)$. (If you would like to have only conditional probabilities in your model, you can imagine the probability of picking the first word as $P(first|START)$, where $START$ is a special token.)\n",
      "\n",
      "All subsequent words are generated with probability $P(w_{t+1}|w_t)$, where $t$ is the *time step*, or position in the sentence. These probabilities are also called *transition* probabilities. If we only base them on the current word, we are essentially using a two-word window, so people say that we use a *first-order* Markov Chain, or - especially for words - a *bigram* chain.\n",
      "\n",
      "We get the probability information from counting the words in a corpus. We use the NLTK library to access the corpus, and the Counter object to get the word counts. Finally, we store the counts as `pandas` data frames (this will make sampling based on specific words easier)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use only the first 20k words and lower-case them\n",
      "corpus = map(str.lower, brown.words()[:20000])\n",
      "\n",
      "# get counts\n",
      "first_word_count = Counter([sentence[0].lower() for sentence in brown.sents()[:100]])\n",
      "first_words, first_counts = zip(*first_word_count.items())\n",
      "P_first = pd.DataFrame(data=[first_words, first_counts]).T\n",
      "P_first.set_index(P_first[0])\n",
      "\n",
      "bigram_counts = Counter(nltk.bigrams(corpus))\n",
      "w1, w2 = zip(*bigram_counts.keys())\n",
      "P_bigram = pd.DataFrame(data=[w1, w2, map(float, bigram_counts.values())]).T\n",
      "P_bigram.set_index(P_bigram[0])\n",
      "\n",
      "final_words = Counter([sentence[-1].lower() for sentence in brown.sents()[:100]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now define some functions that sample from the different distributions we have: the initial word distribution, and the conditional probability distribution over the bigrams.\n",
      "We use the `numpy.random.multinomial` function to pass it a vector of probabilities and sample one position from it, according to the distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sample_first():\n",
      "    total = float(sum(P_first[1]))\n",
      "    P_first[1] /= total\n",
      "    \n",
      "    x = np.argmax(np.random.multinomial(1, np.array(P_first[1], dtype=float)))\n",
      "    return P_first[0][x]\n",
      "\n",
      "\n",
      "def sample_next_bigram_word(word):\n",
      "    #get distro\n",
      "    column = P_bigram[P_bigram[0] == word]\n",
      "    total = float(sum(column[2]))\n",
      "    column[2] = column[2] / total\n",
      "    \n",
      "    # sample from distribution\n",
      "    x = np.argmax(np.random.multinomial(1, np.array(column[2], dtype=float)))\n",
      "    return column[1].values[x]\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can run our code and generate a number of sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate():\n",
      "    result = []\n",
      "    previous_word = None\n",
      "    word = sample_first()\n",
      "    result.append(word)\n",
      "    while word not in final_words:\n",
      "        next_word = sample_next_bigram_word(word)\n",
      "\n",
      "        previous_word = word\n",
      "        word = next_word\n",
      "\n",
      "        result.append(word)\n",
      "\n",
      "    return ' '.join(result)\n",
      "\n",
      "for x in xrange(5):\n",
      "    print x+1, generate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the reference , she is not fully concrete block capitals of his sculpture '' ? ? i'd like a demonstration '' ; the ripples very badly .\n",
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the road '' .\n",
        "3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the sauce , preferably a good as legitimate questions as an inundating joy , braque were strong men .\n",
        "4 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "`` the ghetto walls and mutual inspections to continue the claimant as male accountant who would be well received $1,450,000,000 in ever-tightening circles and their pace as just have a 3-hp. turbine division )\n",
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "during the terrier club also be used it purports to welch's relentless pursuit of the recommendations .\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "The results are somewhere at the intriguing boundary between sensible and complete gibberish (sounds like spam email?). The problem is that the generator only looks at two words at a time (the bigram), and that many phrases and constructions span more than two words. Consequently, the generated sentences \"switch direction\" in mid-stream.\n",
      "\n",
      "The best way to improve this is to include more information about the past than just the current word. How about the previous one as well? If we condition the next word on the previous and the current word, we are using a *trigram* or *second-order* Markov Chain.\n",
      "\n",
      "Exercise 1:\n",
      "----\n",
      "\n",
      "Modify the bigram code from above to get trigram statistics and write a function to sample from it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "\n",
      "# DH REMOVE CODE!!!\n",
      "trigram_counts = Counter(nltk.ngrams(corpus, 3))\n",
      "w1, w2, w3 = zip(*trigram_counts.keys())\n",
      "P_trigram = pd.DataFrame(data=[w1, w2, w3, map(float, trigram_counts.values())]).T\n",
      "P_trigram.set_index(P_trigram[0])\n",
      "\n",
      "def sample_next_trigram_word(word1, word2):\n",
      "    # Your code here\n",
      "    \n",
      "    # DH REMOVE CODE!!!\n",
      "    #get distro\n",
      "    column = P_trigram[(P_trigram[0] == word1) & (P_trigram[1] == word2)]\n",
      "    total = float(sum(column[3]))\n",
      "    column[3] = column[3] / total\n",
      "    \n",
      "    # sample from distribution\n",
      "    x = np.argmax(np.random.multinomial(1, np.array(column[3], dtype=float)))\n",
      "    return column[2].values[x]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 2:\n",
      "----\n",
      "\n",
      "The advantage of trigrams is that it will generate much more sensible text. The disadvantage is that we will never see every combination of three words possible. We can thus get into a situation where we try to condition on the current and previous word, but there exist no statistics on that particular combination. This is called *sparsity*. In order to avoid the generator from breaking, we need to be able to fall back on bigrams whenever the trigram sampling fails.\n",
      "\n",
      "Modify the generator in order to include trigrams, and make sure it can handle the sparsity problem. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate2():\n",
      "    # Your code here\n",
      "    \n",
      "    # DH REMOVE CODE!\n",
      "    result = []\n",
      "    previous_word = None\n",
      "    word = sample_first()\n",
      "    result.append(word)\n",
      "    while word not in final_words:\n",
      "        try:\n",
      "            next_word = sample_next_trigram_word(previous_word, word)\n",
      "\n",
      "        except ValueError:\n",
      "            next_word = sample_next_bigram_word(word)\n",
      "\n",
      "        previous_word = word\n",
      "        word = next_word\n",
      "\n",
      "        result.append(word)\n",
      "\n",
      "    return ' '.join(result)\n",
      "\n",
      "for x in xrange(5):\n",
      "    print x+1, generate2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "henry watterson to sound convinced .\n",
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "five minutes of misfortune that left three , labor and is responsible to no organic disorders in that which can speak both kinds of clothes -- she'd be through here , h and equating to zero .\n",
        "3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "`` where does tim's wife live '' ? ? is this atomic nucleus that contains the positive charge of the city from all over the southern region may be moral in all countries by helping teachers learn to recognize that in man gross deficiency in the securing of such devices , ambassador stevenson warned the un army is part of the lewis killing .\n",
        "4 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "it clear that i didn't have any detrimental effect on american life '' .\n",
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "`` conservatism '' .\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 3:\n",
      "----\n",
      "\n",
      "A side effect of the sparse trigram matrix is that the generator might end up faithfully replicating the original sentence, simply because for any two-word history, there is only one option how to continue (not too many words can follow the bigram \"*appelate court*\"). What would you have to do to overcome this sparsity?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}