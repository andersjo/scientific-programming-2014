{
 "metadata": {
  "name": "",
  "signature": "sha256:fd84bf4255c7cfefc161189c25d49058f09362af9674657d86bab4b9415a6264"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 2: The inverted index\n",
      "\n",
      "The inverted index is the data structure that drives unstructured ad-hoc text retrieval, and it has no real competitors. The inverted index makes searching at web scale (billions of documents) possible and efficient. \n",
      "\n",
      "In this exercise you are going to implement an inverted index and use it to retrieve songs by lyrics from the *One Million Song* data set. Because we use a high-level interpreted langauge and do not pay painstaking attention to code efficiency, the code you develop in this exercise may not be completely ready to take on the whole web. But you will know the basic principles.\n",
      "\n",
      "### Data set \n",
      "\n",
      "Download [the lyrics in BOW format](https://dl.dropboxusercontent.com/u/1423772/mxm_dataset_train.txt), [artist metadata](https://dl.dropboxusercontent.com/u/1423772/mxm_779k_matches.txt), the [stemmed-to-full-form mapping](https://dl.dropboxusercontent.com/u/1423772/mxm_reverse_mapping.txt), and place them in the current directory. \n",
      "\n",
      "As you execute the cell below it will build a dictionary `artists` with information about artist name and title, keyed by track id. It also builds an inverted index `index_terms`. Techically the inverted index is a dictionary where the keys are terms (i.e. words from the songs) and the values are long lists of document identifiers, refering to documents in which the term appears. The list of document identifiers is also called a postings list. \n",
      "\n",
      "Note that it might take a couple of minutes to run the cell. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "from itertools import islice\n",
      "from collections import defaultdict\n",
      "\n",
      "def read_artists_map():\n",
      "    artists_by_mxm = {}\n",
      "    for line in codecs.open(\"mxm_779k_matches.txt\", encoding='utf-8'):\n",
      "        if line[0] in (\"#\", \"%\"):\n",
      "            continue\n",
      "        # Format: tid|artist name|title|mxm tid|artist_name|title\n",
      "        #          0       1        2       3       4         5\n",
      "        parts = line.strip().split(\"<SEP>\")\n",
      "        if len(parts) == 6:\n",
      "            artists_by_mxm[int(parts[3])] = (parts[4], parts[5])\n",
      "    \n",
      "    return artists_by_mxm\n",
      "\n",
      "def read_unstem_map():\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    unstem_map = {}\n",
      "    for line in codecs.open(\"mxm_reverse_mapping.txt\", encoding='utf-8'):\n",
      "        stemmed, orig = line.strip().split(\"<SEP>\")\n",
      "        unstem_map[stemmed] = orig\n",
      "    \n",
      "    return unstem_map\n",
      "        \n",
      "def make_term_index():\n",
      "    unstem_map = read_unstem_map()\n",
      "    terms = defaultdict(list)\n",
      "\n",
      "    words = []\n",
      "    for line in islice(codecs.open(\"mxm_dataset_train.txt\", encoding='utf-8'), None):\n",
      "        if line.startswith(\"#\"):\n",
      "            continue\n",
      "        elif line.startswith(\"%\"):\n",
      "            words = ['*EMPTY*'] + [unstem_map[word] for word in line[1:-1].split(\",\")]\n",
      "\n",
      "        else:\n",
      "            parts = line.split(\",\")\n",
      "            mxm_track_id = int(parts[1])\n",
      "            for part in parts[2:]:\n",
      "                word_id, count = part.split(\":\")\n",
      "                word = words[int(word_id)]\n",
      "                terms[word].append(mxm_track_id)\n",
      "    \n",
      "    for postings in terms.itervalues():\n",
      "        postings.sort()\n",
      "    \n",
      "    return terms\n",
      "            \n",
      "artists = read_artists_map()\n",
      "index_terms = make_term_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Ex. 2.1. Conjunctive query \n",
      "\n",
      "Write a function `query_conj(query)` that accepts a string parameter `query` of white-space separated terms. `query_conj` should intersect the postings lists associated with the query terms and return the results. You should not use the Python `set` operations for this purpose, because this approach will not scale for large data sets. \n",
      "\n",
      "When you three or more query terms, the intersection operation can be decomposed into binary operations involving only two posting lists. For instance, with postings lists $p, q, r$, you could intersect $p$ and $q$ in a temporary list $p'$, which could then be intersected with $r$. So the binary operations in this example would be $p' = p \\cap q$ and $p' \\cap r$.\n",
      "\n",
      "Test the speed of your implementation with the `%%timeit` IPython magic. Place the magic in the top of a cell and write the code you want to have benchmarked below. For instance, \n",
      "\n",
      "```\n",
      "%%timeit\n",
      "query_conj(\"love and gray silver\")\n",
      "\n",
      "```\n",
      "\n",
      "Try queries that involve both large and small intersections. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Ex 2.2 Conjunctive query with binary search\n",
      "\n",
      "In the exercise above you might have noticed that a lot of unnecessary work is done when intersecting a small postings list like that for *pan* and huge ones for common words like *and*. In a nutshell the problem is that the large list has an entry for almost every document, while there may be thousands of documents between entries in the smaller list. And yet we dutifully check every entry in the larger list against the smaller list. Is it possible to skip ahead?\n",
      "\n",
      "Yes, because both lists are sorted. We can thus jump some distance ahead in the larger list, checking 1) whether we overshoot, 2) are still not far enough ahead, or 3) perhaps got lucky and landed exactly at the correct element in larger list. In case of 1) or 2), we make another jump to get closer to the correct element. In case the jump overshot we know that the value we are searching for must be between the initial value and the place we landed. This gives us a search space. If every jump halves the search space this approach is called binary search. It may happen that the larger list does not contain the current entry of the smaller list. We can detect this situation when the search space spans nothing. \n",
      "\n",
      "Binary search may be one of the simplest algorithm out there, but it can be surprisingly hard to get right. \n",
      "\n",
      "Implement a conjunctive query with binary search `query_conj_bs` and test the performance against `query_conj` on several different queries. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    }
   ],
   "metadata": {}
  }
 ]
}