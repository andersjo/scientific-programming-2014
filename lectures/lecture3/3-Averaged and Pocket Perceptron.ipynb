{
 "metadata": {
  "name": "",
  "signature": "sha256:c1e410da339eed72dbe634a895d645398726db05fcf3459470bb0d63b823e896"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 3\n",
      "====\n",
      "\n",
      "In this exercise, you implement two common variants of the perceptron that can improve performance. Copy over your code from one of the previous exercises and modify it to produce the variants. You can use separate copies of the code for each variant, or implement a version that takes a parameter to decide what to use."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variant 1: Average perceptron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Often the final decision boundary produced by the perceptron algorithm on the training set will result in only mediocre performance on a test set. This is true both when the algorithm finds a line that perfectly separates the two classes (and thus obtains 100% accuracy on the training set), and when such a line cannot be found, perhaps because the classes are not linearly separable. \n",
      "\n",
      "When the classes are separable the \"perfect\" line may pass too close to some instances, at the risk of misclassifying similar though not identical instances in the test set. In that case we say the classifier is *overfit*, because it relies on quirks of the training set and cannot generalize to new instances. Ideally the line should be some distance away from the instances, leaving a *margin*. \n",
      "\n",
      "When the classes are not separable, the perceptron algorithm still tries as hard as it can to find the line, even if it eventually must give up and return whatever the current line is. Thus the final decision boundary is not necessarily the best one seen during the training.  \n",
      "\n",
      "Rather than taking the final weight vector to classify new instances, we can use information from the entire training process. One way is to classify each new instance with *all* weight vectors we produced during training, and then see how often it ends up with one class or the other. This is called the *voted* perceptron, and it is slightly messy. A more practical approximation is the **averaged** perceptron, which you will implement, where we use the average over all different weight vectors we have seen in training.\n",
      "\n",
      "Hint: Save *a copy* of the current weight vector, including bias term, each time the weight vector is updated. When the training process has completed, produce a new weight vector as the average of all saved vectors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variant 2: Pocket perceptron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another variant is the **pocket** perceptron, which keeps the best weight vector it has seen during training in its \"pocket\" and returns that as the final weight vector.\n",
      "\n",
      "Hint: Best here means the weight vector with the highest classification accuracy on the training set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}