{
 "metadata": {
  "name": "",
  "signature": "sha256:efad2381851921b1c7e93c51f1e153132e413ddf66044f2862fa82dbd60a14dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from collections import defaultdict\n",
      "import codecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hidden Markov Models\n",
      "====\n",
      "\n",
      "We've talked before about Markov Chains, and mentioned that they are the basis of Hidden Markov Models (HMMs). Today, you will work with an HMM.\n",
      "\n",
      "The observed sequences are words, and the hidden sequences are the consonant-vowel or syllable structure of the word. This is not as easy as assigning `vowel` to all As, Es, Is, etc., and `consonant` to the rest, since English has a lot of \"silent\" letters, such as the *e* in lik*e*, which would be assigned to the consonant.\n",
      "\n",
      "As you have seen before with other models, the HMM class has two functions: `fit()` reads in a file with a letter and the corresponding label (C or V). Words are separated by empty lines.\n",
      "The model keeps track of three objects:\n",
      "\n",
      "1. the word initial label probabilities, \n",
      "2. the transition probabilities from label to label,\n",
      "3. the emission probabilities from label to letter\n",
      "\n",
      "It simply counts each occurrence and then normalizes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HMM(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.initial = None\n",
      "        self.emissions = None\n",
      "        self.transitions = None\n",
      "        self.labels = set()\n",
      "    \n",
      "    \n",
      "    def read_file(self, file_name):\n",
      "        \"\"\"\n",
      "        read in a CoNLL file: first column=words, second column(optional)=tags\n",
      "        word1    tag1\n",
      "        word2    tag2\n",
      "        ...      ...\n",
      "        wordN    tagN\n",
      "        \n",
      "        Needs to end on an empty line!\n",
      "        \"\"\"\n",
      "        current_instance = []\n",
      "\n",
      "        for line in codecs.open(file_name, encoding='utf-8'):\n",
      "            line = line.strip()\n",
      "            # if line is not empty, \n",
      "            if line:\n",
      "                if '\\t' in line:\n",
      "                    current_instance.append(tuple(line.split('\\t', 1)))\n",
      "                else:\n",
      "                    current_instance.append((line, None))\n",
      "\n",
      "            # otherwise, return current instance\n",
      "            else:\n",
      "                yield current_instance\n",
      "                \n",
      "                # reset current instance\n",
      "                current_instance = []\n",
      "    \n",
      "    \n",
      "    def fit(self, file_name):\n",
      "        \"\"\"\n",
      "        fit the model from a CoNLL-formatted file\n",
      "        \"\"\"\n",
      "        initial_counts = defaultdict(float)\n",
      "        emission_counts = defaultdict(lambda: defaultdict(float))\n",
      "        transition_counts = defaultdict(lambda: defaultdict(float))\n",
      "        \n",
      "        # record parameters and labels\n",
      "        for instance in self.read_file(file_name):\n",
      "            initial_counts[instance[0][1]] += 1\n",
      "            self.labels.add(instance[0][1])\n",
      "            \n",
      "            for i, (word, tag) in enumerate(instance):\n",
      "                self.labels.add(tag)\n",
      "                emission_counts[tag][word] += 1\n",
      "                if i == 0:\n",
      "                    previous_tag = \"START\"\n",
      "                else:\n",
      "                    previous_tag = instance[i-1][1]\n",
      "                transition_counts[previous_tag][tag] += 1\n",
      "\n",
      "        self.labels = sorted(list(self.labels))\n",
      "                \n",
      "        # normalize counts to probabilities\n",
      "        initial_total = sum(initial_counts.values())\n",
      "        for tag in initial_counts:\n",
      "            initial_counts[tag] /= initial_total\n",
      "            \n",
      "        for tag, words in emission_counts.iteritems():\n",
      "            tag_total = sum(words.values())\n",
      "            for word in words:\n",
      "                emission_counts[tag][word] /= tag_total\n",
      "        \n",
      "        for tag1, tags in transition_counts.iteritems():\n",
      "            tag_total = sum(tags.values())\n",
      "            for tag2 in tags:\n",
      "                transition_counts[tag1][tag2] /= tag_total\n",
      "            \n",
      "        # transfer to DataFrames\n",
      "        self.initial = pd.DataFrame.from_dict(initial_counts, orient='index')\n",
      "        self.emissions = pd.DataFrame.from_dict(emission_counts, orient='index')\n",
      "        self.transitions = pd.DataFrame.from_dict(transition_counts, orient='index')\n",
      "                \n",
      "        \n",
      "    def predict(self, instance):\n",
      "        \"\"\"\n",
      "        implements the Viterbi Algorithm to predict labels for a sequence of words\n",
      "        \"\"\"\n",
      "        predicted_labels = []\n",
      "        \n",
      "        # YOUR CODE HERE\n",
      "        \n",
      "        return predicted_labels\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 1:\n",
      "----\n",
      "Look at the parameter tables in fit(). LOOK AT THEM!!! Like, up close and personal. Make a plot and see whether all parameters are represented equally.\n",
      "\n",
      "Are there empty cells?\n",
      "\n",
      "Which characters can occur as C and V?\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 2:\n",
      "----\n",
      "\n",
      "Implement the Viterbi Algorithm as the `predict()` function *in the cell above*. It takes a list as input and should also return a list. Use the words in `cv.test` to see your predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = HMM()\n",
      "h.fit('cv.train')\n",
      "\n",
      "# YOUR CODE HERE\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 3:\n",
      "----\n",
      "\n",
      "Implement smoothing in `fit()` by adding 1 to each possible parameter (you first need to know what all the labels and observatinos are)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 4:\n",
      "----\n",
      "\n",
      "Implement a function to measure the accuracy of a predicted sequence as compared to the gold standard. Use the instances in `cv.test` "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 5 (optional):\n",
      "----\n",
      "\n",
      "Currently, the HMM is a first-order HMM, i.e., the hidden chain (the tags) are a bigram model, where each tag only depends on the one before it. Change the code so that you can choose the order.\n",
      "\n",
      "* add a parameter `order=2` to the constructor, so that it defaults to the first order/bigram model\n",
      "* modify the fitting of the transition parameters in `fit()` so that the model can take any order into account\n",
      "* adjust the `predict()` function"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}