{
 "metadata": {
  "name": "",
  "signature": "sha256:1f05f6496c18c47621e8b47eafbbe80a2ca319f14c27c1c8c474850ca85ef7a7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from collections import defaultdict\n",
      "import codecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hidden Markov Models\n",
      "====\n",
      "\n",
      "We've talked before about Markov Chains, and mentioned that they are the basis of Hidden Markov Models (HMMs). Today, you will work with an HMM.\n",
      "\n",
      "The observed sequences are words, and the hidden sequences are the consonant-vowel or syllable structure of the word. This is not as easy as assigning `vowel` to all As, Es, Is, etc., and `consonant` to the rest, since English has a lot of \"silent\" letters, such as the *e* in lik*e*, which would be assigned to the consonant.\n",
      "\n",
      "As you have seen before with other models, the HMM class has two functions: `fit()` reads in a file with a letter and the corresponding label (C or V). Words are separated by empty lines.\n",
      "The model keeps track of three objects:\n",
      "\n",
      "1. the word-initial label probabilities, \n",
      "2. the transition probabilities from label to label,\n",
      "3. the emission probabilities from label to letter.\n",
      "\n",
      "The function simply counts each occurrence and then normalizes to form probabilities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HMM(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.initial = None\n",
      "        self.emissions = None\n",
      "        self.transitions = None\n",
      "        self.labels = set()\n",
      "    \n",
      "    \n",
      "    def read_file(self, file_name):\n",
      "        \"\"\"\n",
      "        read in a CoNLL file: first column=words, second column(optional)=tags\n",
      "        word1    tag1\n",
      "        word2    tag2\n",
      "        ...      ...\n",
      "        wordN    tagN\n",
      "        \n",
      "        Needs to end on an empty line!\n",
      "        \"\"\"\n",
      "        current_instance = []\n",
      "\n",
      "        for line in codecs.open(file_name, encoding='utf-8'):\n",
      "            line = line.strip()\n",
      "            # if line is not empty, \n",
      "            if line:\n",
      "                if '\\t' in line:\n",
      "                    current_instance.append(tuple(line.split('\\t', 1)))\n",
      "                else:\n",
      "                    current_instance.append((line, None))\n",
      "\n",
      "            # otherwise, return current instance\n",
      "            else:\n",
      "                yield current_instance\n",
      "                \n",
      "                # reset current instance\n",
      "                current_instance = []\n",
      "    \n",
      "    \n",
      "    def fit(self, file_name):\n",
      "        \"\"\"\n",
      "        fit the model from a CoNLL-formatted file\n",
      "        \"\"\"\n",
      "        initial_counts = defaultdict(float)\n",
      "        emission_counts = defaultdict(lambda: defaultdict(float))\n",
      "        transition_counts = defaultdict(lambda: defaultdict(float))\n",
      "        \n",
      "        # record parameters and labels\n",
      "        for instance in self.read_file(file_name):\n",
      "            initial_counts[instance[0][1]] += 1\n",
      "            self.labels.add(instance[0][1])\n",
      "            \n",
      "            for i, (word, tag) in enumerate(instance):\n",
      "                self.labels.add(tag)\n",
      "                emission_counts[tag][word] += 1\n",
      "                if i == 0:\n",
      "                    previous_tag = \"START\"\n",
      "                else:\n",
      "                    previous_tag = instance[i-1][1]\n",
      "                transition_counts[previous_tag][tag] += 1\n",
      "\n",
      "        self.labels = sorted(list(self.labels))\n",
      "                \n",
      "        # normalize counts to probabilities\n",
      "        initial_total = sum(initial_counts.values())\n",
      "        for tag in initial_counts:\n",
      "            initial_counts[tag] /= initial_total\n",
      "            \n",
      "        for tag, words in emission_counts.iteritems():\n",
      "            tag_total = sum(words.values())\n",
      "            for word in words:\n",
      "                emission_counts[tag][word] /= tag_total\n",
      "        \n",
      "        for tag1, tags in transition_counts.iteritems():\n",
      "            tag_total = sum(tags.values())\n",
      "            for tag2 in tags:\n",
      "                transition_counts[tag1][tag2] /= tag_total\n",
      "            \n",
      "        # transfer to DataFrames\n",
      "        self.initial = pd.DataFrame.from_dict(initial_counts, orient='index')\n",
      "        self.emissions = pd.DataFrame.from_dict(emission_counts, orient='index')\n",
      "        self.transitions = pd.DataFrame.from_dict(transition_counts, orient='index')\n",
      "        \n",
      "        \n",
      "    def predict(self, instance):\n",
      "        \"\"\"\n",
      "        implements the Viterbi Algorithm to predict labels for a sequence of words\n",
      "        \"\"\"\n",
      "        predicted_labels = []\n",
      "        \n",
      "        # Your code here\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 1:\n",
      "----\n",
      "\n",
      "Implement the Viterbi Algorithm as the `predict()` function. It takes a single observation sequence as input and should return a list of predicted tags."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! head cv.data\n",
      "h = HMM()\n",
      "h.fit('cv.data')\n",
      "\n",
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 2:\n",
      "----\n",
      "\n",
      "Currently, the HMM is a first-order HMM, i.e., the hidden chain (the tags) are a bigram model, where each tag only depends on the one before it. Change the code so that you can choose the order.\n",
      "\n",
      "* add a parameter `order=2` to the constructor, so that it defaults to the first order/bigram model\n",
      "* modify the fitting of the transition parameters in `fit()` so that the model can take any order into account\n",
      "* adjust the `predict()` function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}