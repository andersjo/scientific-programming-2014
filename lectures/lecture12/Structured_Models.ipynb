{
 "metadata": {
  "name": "",
  "signature": "sha256:3cf6ae30b3d1ecf398758607987fb23e473c38d9222957334be658347cc4450d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Structured Models\n",
      "====\n",
      "\n",
      "So far, we have only looked at classification tasks that ouput *one* label for each instance. In structured labeling, we want instead a sequence of labels. Training a classification model with one output label for each possible sequence is typically impossible, and even if possible, typically has low performance. For another thing we get from sequence models is information based on the surroundings, i.e., labels depend upon the contxt, i.e, each other. In the following example, the words *Paris* and *Hilton* can refer either to a person or a location, depending on the context:\n",
      "<table>\n",
      "<tr><td>PER</td><td>PER</td><td>O</td><td>O</td><td>O</tr>\n",
      "<tr><td>Paris</td><td>Hilton</td><td>has</td><td>no</td><td>clue</td></tr>\n",
      "</table>\n",
      "<table>\n",
      "<tr><td>PER</td><td>O</td><td>O</td><td>O</td><td>LOC</td><td>in</td><td>LOC</td></tr>\n",
      "<tr><td>He</td><td>stayed</td><td>at</td><td>the</td><td>Hilton</td><td>in</td><td>Paris</td></tr>\n",
      "</table>\n",
      "\n",
      "The way we model this is by using a Markov chain. Each label depends on the previous one (or more, if it is a higher-order chain).\n",
      "In addition, we add some more nodes, which represent the words. They depend on the labels as well, i.e., each word is \"output\" by a label with some probability.\n",
      "\n",
      "<img src=hmm.png?02 width=600px>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decoding\n",
      "====\n",
      "\n",
      "In order to find the best underlying label sequence (e.g., POS tags, phonemes, genes, etc.) for an observed sequence (words, sounds, proteins), in theory we need to consider all possible labelings and choose the one with the highest likelihood. \n",
      "\n",
      "E.g., for a mini example like \"mice like cheese\" and just two tags (NOUN, VERB), we would have to consider 8 possible labelings or paths:\n",
      "\n",
      "1. NOUN, NOUN, NOUN\n",
      "2. NOUN, NOUN, VERB\n",
      "3. NOUN, VERB, NOUN\n",
      "4. NOUN, VERB, VERB\n",
      "5. VERB, NOUN, NOUN\n",
      "6. VERB, NOUN, VERB\n",
      "7. VERB, VERB, NOUN\n",
      "8. VERB, VERB, VERB\n",
      "\n",
      "For each paths, we would multiply together the probabilities that are involved to get a score, and then pick the one with the maximum.\n",
      "\n",
      "If we did this naively, always starting at the first position and computing our way through teh path, we would do a lot of redundant computations (1. and 2. are the same except for the last position, i.e., we'd re-use all emission and transition probabilities up to there). This isn't too bad if we have only 8 possible paths, but as our numbers of labels and the lengths of the sequences grow, this quickly becomes infeasible.\n",
      "\n",
      "Instead, we use the principles of dynamic programming: Remember what we have computed up to a certain point, and only add what's new.\n",
      "\n",
      "Lattices\n",
      "----\n",
      "\n",
      "In order to represent all the possible taggings for a sequence, the most frequently used representation is a ***lattice*** (sometimes also called a trellis). \n",
      "\n",
      "It's a state graph, where each node is the combination of an observation (e.g., a word) with a label (e.g., a POS tag). In addition, there are start and end nodes. The end node is also used to compute the overall likelihood of the observation.\n",
      "\n",
      "The easiest way to represent a lattice is to use an $N$-by-$T$ matrix, where $N$ is the number of labels we have, and $T$ is the length of the sequence we want to tag ($T$ because each element of the sequence is called a ***t***ime step).\n",
      "Here is the lattice for our little example\n",
      "<img src=trellis.png width=600px/>\n",
      "\n",
      "In order to find the best label sequence, we compute for each step how likely each label is, and choose the best one. The question of which label is the best is solved by decoding.\n",
      "\n",
      "\n",
      "\n",
      "The Viterbi Algorithm\n",
      "----\n",
      "\n",
      "<img src=andrew_viterbi.jpg width=200px/>\n",
      "\n",
      "The Viterbi algorithm is named after Andrew Viterbi. To compute how likely each tag is for a word, we use two probabilities:\n",
      "\n",
      "1. the probability of seeing this tag after having observed a certain previous tag. This is called the ***transition probability***, $P(t_j|t_i)$, because we transition from one state/tag to the next. This is exactly the probabilities we have seen and used in the Markov chains\n",
      "\n",
      "2. the probability that the word receives that particular tag. This is called the ***emission probability***, $P(w_i|t_i)$, because we imagine that the state/tag emits the word.\n",
      "\n",
      "We use two matrices to represent the path through the lattice: $Q$ holds the likelihood of the shortest path ending with word $i$ getting assigned tag $j$. The other matrix, $V$ keeps track of which tag is the predecessor on that path. \n",
      "\n",
      "In $Q$, the score in each cell is the sum of all path from all cells in the previous column to this one, each mulitplied by the appropriate transition probability, and the emission probability of the current word.\n",
      "\n",
      "$$Q[j,i] = \\text{max}_k\\ Q[k, i-1] \\times P(tags_j|tags_k) \\times P(words_i|tags_i)$$\n",
      "\n",
      "Once we have filled both matrices, we use $Q$ to find the highest scoring tag for the last word, and then simply follow the back-pointers in $V$ until we've reached the beginning. Finally, we reverse that sequence (it's backwards), and that's it.\n",
      "\n",
      "In pseudocode, we can write it as follows:\n",
      "\n",
      "<pre>\n",
      "Q = zeros(|tags|, |words|)\n",
      "V = zeros(|tags|, |words|)\n",
      "\n",
      "# initialize first column\n",
      "for j in |tags|:\n",
      "    Q[j, 0] = P(tags_j|START) * P(words_0|tags_j)\n",
      "# fill lattice\n",
      "for i in |words|:\n",
      "    for j in |tags|:\n",
      "        best_score = -inf\n",
      "        for k in |tags|:\n",
      "            score = Q[k, i-1] * P(tags_j|tags_k) * P(words_i|tags_i)\n",
      "            if score > best_score:\n",
      "                best_score = score\n",
      "                V[j, i] = k\n",
      "                Q[j, i] = score\n",
      "                \n",
      "# find best path, starting at the back\n",
      "viterbi_path = [argmax[Q[:, -1]]]\n",
      "for i in |words|...-1:\n",
      "    viterbi_path.append(V[viterbi_path[-1], i+1])\n",
      "    \n",
      "return reversed(viterbi_path)\n",
      "</pre>\n",
      "\n",
      "Our two matrices in the example might look like this (for $V$, the best path NOUN-VERB-NOUN is indicated):\n",
      "\n",
      "<img src=Q.png width=500px \\><img src=V.png width=500px \\>\n",
      "\n",
      "\n",
      "Posterior Decoding\n",
      "----\n",
      "\n",
      "Viterbi is not the only algorithm to find the best decoded sequence: Posterior Decoding is a viable alternative. To get it, you need to compute a forward and a backward pass through the lattice. The forward pass is the same as the Viterbi algorithm, but instead of the ***max***, we store the ***sum*** of all path that lead to each node, and we don't need to keep track of the best predecessor.\n",
      "\n",
      "The backward algorithm is the same as the forward algorithm, but starts at the end and works its way backwards through the lattice. (This is similar to the difference between depth-first and breadth-first search).\n",
      "If you mulity the forward and backward probabilities of each node, you get its posterior probability. Now you just pick for each word the label with the highest probability.\n",
      "\n",
      "Computing the posterior probability thus involves twice the number of operations as Viterbi (because you need to go over the lattice forwards and backwards), and it can potentially pick a sequence of labels whose transition probability is 0.0, but in practice, it often does better (in terms of accuracy) than Viterbi.\n",
      "\n",
      "Here is pseudocode for the forward algorithm\n",
      "<pre>\n",
      "alpha = zeros(|tags|, |words|)\n",
      "\n",
      "# initialize first column\n",
      "for j in |tags|:\n",
      "    alpha[j, 0] = P(tags_j|START) * P(words_0|tags_j)\n",
      "# fill lattice\n",
      "for i in |words|:\n",
      "    for j in |tags|:\n",
      "        for k in |tags|:\n",
      "            alpha[j, i] += alpha[k, i-1] * P(tags_j|tags_k) * P(words_i|tags_i)\n",
      "            \n",
      "# compute likelihood of word sequence\n",
      "P(words) = 0\n",
      "for j in |tags|:\n",
      "    P(words) += alpha[j, -1]\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}